{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a conversation with SystemMessage, AIMessage and SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Why did the cow cross the road?\\nTo get to the **udder** side!' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019c896c-dcf4-7951-ab7e-163a8130e47b-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 7, 'output_tokens': 1416, 'total_tokens': 1423, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 1398}}\n"
     ]
    }
   ],
   "source": [
    "result = llm.invoke(\"Tell me a joke about cows\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant specialized in providing information about BellaVista Italian Restaurant.\"),\n",
    "    HumanMessage(content=\"What's on the menu?\"),\n",
    "    AIMessage(content=\"BellaVista offers a variety of Italian dishes including pasta, pizza, and seafood.\"),\n",
    "    HumanMessage(content=\"Do you have vegan options?\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Yes, BellaVista is happy to accommodate various dietary preferences, including vegan options. Many of our pasta dishes can be made vegan by omitting cheese or using plant-based alternatives where possible, and we also offer fresh salads and vegetable-based appetizers.\\n\\nOur chefs are often able to adapt dishes to suit your needs. We recommend mentioning your vegan preference to your server when you visit so they can guide you through the best choices available and discuss any modifications.', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c896a-761f-74d1-bff1-ceb28426fc72-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 47, 'output_tokens': 766, 'total_tokens': 813, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 676}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_result = llm.invoke(input=messages)\n",
    "llm_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Processing for Chat Models - Make multiple requests at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Haben Sie vegane Optionen?', generation_info={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, message=AIMessage(content='Haben Sie vegane Optionen?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c896d-bb83-78b1-9d5c-82db39138d16-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 17, 'output_tokens': 529, 'total_tokens': 546, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 522}}))], [ChatGeneration(text='¿Tienen opciones veganas?', generation_info={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, message=AIMessage(content='¿Tienen opciones veganas?', additional_kwargs={}, response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': [], 'model_provider': 'google_genai'}, id='lc_run--019c896d-bb83-78b1-9d5c-82e10b587bb2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 19, 'output_tokens': 275, 'total_tokens': 294, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 268}}))]], llm_output={}, run=[RunInfo(run_id=UUID('019c896d-bb83-78b1-9d5c-82db39138d16')), RunInfo(run_id=UUID('019c896d-bb83-78b1-9d5c-82e10b587bb2'))], type='LLMResult')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates English to German\"),\n",
    "        HumanMessage(content=\"Do you have vegan options?\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are a helpful assistant that translates the English to Spanish.\"),\n",
    "        HumanMessage(content=\"Do you have vegan options?\")\n",
    "    ],\n",
    "]\n",
    "batch_result = llm.generate(batch_messages)\n",
    "batch_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracting the output ourself (we will later take a look at Output-Parsers!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Haben Sie vegane Optionen?', '¿Tienen opciones veganas?']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translations = [generation[0].text for generation in batch_result.generations]\n",
    "translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangChain 1.0 - Content blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "response = model.invoke(\"What's the capital of France?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "print(response.tool_calls)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text', 'text': 'The capital of France is **Paris**.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The capital of France is **Paris**.\n"
     ]
    }
   ],
   "source": [
    "for block in response.content_blocks:\n",
    "    if block[\"type\"] == \"reasoning\":\n",
    "        print(f\"Model reasoning: {block['reasoning']}\")\n",
    "    elif block[\"type\"] == \"text\":\n",
    "        print(f\"Response: {block['text']}\")\n",
    "    elif block[\"type\"] == \"tool_call\":\n",
    "        print(f\"Tool call: {block['name']}({block['args']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
