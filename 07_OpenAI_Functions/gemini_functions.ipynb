{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gemini Function Calling\n",
    "\n",
    "Google's Gemini models support function calling (also known as tool use), which allows the LLM to identify when a function should be called and provide parameters for that function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rishu.kumar\\Documents\\0011_OFFICE_FOLDER\\Udemy Courses\\LangChain-Udemy-Course\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\rishu.kumar\\AppData\\Local\\Temp\\ipykernel_29504\\2135204452.py:1: FutureWarning: \n",
      "\n",
      "All support for the `google.generativeai` package has ended. It will no longer be receiving \n",
      "updates or bug fixes. Please switch to the `google.genai` package as soon as possible.\n",
      "See README for more details:\n",
      "\n",
      "https://github.com/google-gemini/deprecated-generative-ai-python/blob/main/README.md\n",
      "\n",
      "  import google.generativeai as genai\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The cost of pizza salami can vary quite a bit depending on several factors:\\n\\n**Factors Influencing Pizza Salami Cost:**\\n\\n* **Type of Salami:**\\n    * **Standard Pepperoni:** This is the most common and generally the most affordable.\\n    * **Spicy Salami (e.g., Calabrese, Hot Sopressata):** These often have a bit more kick and can be slightly more expensive.\\n    * **Gourmet or Artisanal Salami:** Salami made with higher quality ingredients, unique spice blends, or traditional curing methods will be the most expensive. Examples include Finocchiona, Genoa Salami, or specific regional varieties.\\n    * **Lower Quality Salami:** Sometimes, very cheap pizzas might use a less flavorful, processed salami.\\n\\n* **Brand:** Well-known and established brands might command a slightly higher price than generic or store-brand options.\\n\\n* **Where You Buy It:**\\n    * **Supermarket:** You\\'ll find a wide range of prices here, from budget-friendly to premium.\\n    * **Specialty Deli/Butcher Shop:** You\\'ll likely find higher quality and a wider variety, but also at a higher price point.\\n    * **Restaurant (as part of a pizza):** The cost is integrated into the overall pizza price.\\n\\n* **Form (Whole Salami vs. Sliced):**\\n    * **Whole Salami:** Buying a whole stick of salami is often more cost-effective per pound than buying pre-sliced.\\n    * **Pre-sliced Salami:** This is more convenient for pizza topping but you\\'re paying for the processing.\\n\\n* **Location/Region:** Prices can differ based on your geographical location due to local market conditions and shipping costs.\\n\\n**General Price Ranges (as an ingredient, not part of a pre-made pizza):**\\n\\n* **Budget/Standard Pepperoni:** You might find pre-sliced pepperoni for around **$3 - $6 per 5-8 ounce package** at a supermarket. A whole stick might range from **$8 - $15 per pound**.\\n* **Mid-Range/Slightly Higher Quality Salami:** Prices could be from **$6 - $10 per 5-8 ounce package** of pre-sliced, or **$15 - $25 per pound** for a whole stick.\\n* **Gourmet/Artisanal Salami:** Expect to pay **$10 - $20+ per 5-8 ounce package** of pre-sliced, or **$30 - $50+ per pound** for a whole, high-end salami.\\n\\n**If you\\'re asking about the cost of pizza salami *on a pizza*:**\\n\\nThis is even harder to quantify without knowing the specific pizza. The cost of salami is a component of the overall pizza price. You might see a pepperoni pizza priced slightly higher than a plain cheese pizza, but the difference is usually a few dollars, not a separate line item for \"salami cost.\"\\n\\n**To get a more specific answer, you need to tell me:**\\n\\n* **What kind of salami are you interested in?** (e.g., pepperoni, spicy, gourmet)\\n* **Are you looking to buy it as an ingredient or as part of a pizza?**\\n* **Where are you located (generally)?** (e.g., USA, UK, a specific city)'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Configure the Gemini API\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "def chat(query):\n",
    "    model = genai.GenerativeModel(\"gemini-2.5-flash-lite\")\n",
    "    response = model.generate_content(query)\n",
    "    return response.text\n",
    "\n",
    "query = \"How much does pizza salami cost?\"\n",
    "message = chat(query)\n",
    "message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make use of Function calling you need:\n",
    "\n",
    "1. A function\n",
    "2. A dictionary which describes the function (function declaration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def get_pizza_info(pizza_name: str):\n",
    "    \"\"\"Get name and price of a pizza of the restaurant.\"\"\"\n",
    "    pizza_info = {\n",
    "        \"name\": pizza_name,\n",
    "        \"price\": \"10.99\",\n",
    "    }\n",
    "    return json.dumps(pizza_info)\n",
    "\n",
    "# Define the function declaration for Gemini\n",
    "get_pizza_info_declaration = {\n",
    "    \"name\": \"get_pizza_info\",\n",
    "    \"description\": \"Get name and price of a pizza of the restaurant\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"pizza_name\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The name of the pizza, e.g. Salami\",\n",
    "            },\n",
    "        },\n",
    "        \"required\": [\"pizza_name\"],\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I can only provide information about pizzas. Would you like to know about any pizza? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat_with_functions(query):\n",
    "    model = genai.GenerativeModel(\n",
    "        \"gemini-2.5-flash-lite\",\n",
    "        tools=[{\n",
    "            \"function_declarations\": [get_pizza_info_declaration]\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    chat = model.start_chat()\n",
    "    response = chat.send_message(query)\n",
    "    \n",
    "    return response, chat\n",
    "\n",
    "# Test with a non-function query\n",
    "response, chat = chat_with_functions(\"What is the capital of france?\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: response:\n",
      "GenerateContentResponse(\n",
      "    done=True,\n",
      "    iterator=None,\n",
      "    result=protos.GenerateContentResponse({\n",
      "      \"candidates\": [\n",
      "        {\n",
      "          \"content\": {\n",
      "            \"parts\": [\n",
      "              {\n",
      "                \"function_call\": {\n",
      "                  \"name\": \"get_pizza_info\",\n",
      "                  \"args\": {\n",
      "                    \"pizza_name\": \"Salami\"\n",
      "                  }\n",
      "                }\n",
      "              }\n",
      "            ],\n",
      "            \"role\": \"model\"\n",
      "          },\n",
      "          \"finish_reason\": \"STOP\",\n",
      "          \"index\": 0\n",
      "        }\n",
      "      ],\n",
      "      \"usage_metadata\": {\n",
      "        \"prompt_token_count\": 67,\n",
      "        \"candidates_token_count\": 20,\n",
      "        \"total_token_count\": 87\n",
      "      },\n",
      "      \"model_version\": \"gemini-2.5-flash-lite\"\n",
      "    }),\n",
      ")\n",
      "\n",
      "Function call: name: \"get_pizza_info\"\n",
      "args {\n",
      "  fields {\n",
      "    key: \"pizza_name\"\n",
      "    value {\n",
      "      string_value: \"Salami\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test with a function query\n",
    "query = \"How much does pizza salami cost?\"\n",
    "response, chat = chat_with_functions(query)\n",
    "\n",
    "print(\"Response:\", response)\n",
    "print(\"\\nFunction call:\", response.candidates[0].content.parts[0].function_call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pizza name: Salami\n",
      "Function response: {\"name\": \"Salami\", \"price\": \"10.99\"}\n"
     ]
    }
   ],
   "source": [
    "# Extract function call and execute\n",
    "function_call = response.candidates[0].content.parts[0].function_call\n",
    "\n",
    "if function_call.name == \"get_pizza_info\":\n",
    "    pizza_name = function_call.args.get(\"pizza_name\")\n",
    "    print(f\"Pizza name: {pizza_name}\")\n",
    "    \n",
    "    function_response = get_pizza_info(pizza_name=pizza_name)\n",
    "    print(f\"Function response: {function_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final response: The Salami pizza costs 10.99.\n"
     ]
    }
   ],
   "source": [
    "# Send the function result back to the model\n",
    "response = chat.send_message(\n",
    "    content={\n",
    "        \"parts\": [{\n",
    "            \"function_response\": {\n",
    "                \"name\": \"get_pizza_info\",\n",
    "                \"response\": json.loads(function_response)\n",
    "            }\n",
    "        }]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Final response:\", response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using LangChain with Gemini Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, FunctionMessage\n",
    "\n",
    "# Initialize the LLM\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
    "\n",
    "# Define the tool using the @tool decorator\n",
    "@tool\n",
    "def get_pizza_info(pizza_name: str) -> str:\n",
    "    \"\"\"Get name and price of a pizza of the restaurant.\n",
    "    \n",
    "    Args:\n",
    "        pizza_name: The name of the pizza, e.g. Salami\n",
    "    \n",
    "    Returns:\n",
    "        A JSON string containing the name and price of the pizza.\n",
    "    \"\"\"\n",
    "    pizza_info = {\n",
    "        \"name\": pizza_name,\n",
    "        \"price\": \"10.99\",\n",
    "    }\n",
    "    return json.dumps(pizza_info)\n",
    "\n",
    "tools = [get_pizza_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_pizza_info', 'arguments': '{\"pizza_name\": \"Salami\"}'}} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019c9317-3ad1-7e41-97e8-6db75ee1cd49-0' tool_calls=[{'name': 'get_pizza_info', 'args': {'pizza_name': 'Salami'}, 'id': '7131c55b-97ac-4053-aec4-3f981bd380a7', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 109, 'output_tokens': 20, 'total_tokens': 129, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "# Bind tools to the LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Create a prompt template\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can provide information about pizzas.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "# Create the chain\n",
    "chain = prompt | llm_with_tools\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\"input\": \"How much does pizza salami cost?\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool calls: [{'name': 'get_pizza_info', 'args': {'pizza_name': 'Salami'}, 'id': '7131c55b-97ac-4053-aec4-3f981bd380a7', 'type': 'tool_call'}]\n",
      "Function response: {\"name\": \"Salami\", \"price\": \"10.99\"}\n"
     ]
    }
   ],
   "source": [
    "# Check if the model wants to call a function\n",
    "if result.tool_calls:\n",
    "    print(\"Tool calls:\", result.tool_calls)\n",
    "    \n",
    "    # Execute the function\n",
    "    for tool_call in result.tool_calls:\n",
    "        if tool_call[\"name\"] == \"get_pizza_info\":\n",
    "            function_response = get_pizza_info.invoke(tool_call[\"args\"])\n",
    "            print(f\"Function response: {function_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final result: The Salami pizza costs 10.99.\n"
     ]
    }
   ],
   "source": [
    "# Create a more complete chain with function execution\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "def execute_tools(msg):\n",
    "    \"\"\"Execute tool calls from the model's response.\"\"\"\n",
    "    tool_results = []\n",
    "    for tool_call in msg.tool_calls:\n",
    "        if tool_call[\"name\"] == \"get_pizza_info\":\n",
    "            result = get_pizza_info.invoke(tool_call[\"args\"])\n",
    "            tool_results.append(\n",
    "                ToolMessage(\n",
    "                    content=result,\n",
    "                    tool_call_id=tool_call[\"id\"]\n",
    "                )\n",
    "            )\n",
    "    return tool_results\n",
    "\n",
    "# Chain that handles function calling\n",
    "def chat_with_tools(query):\n",
    "    # First call\n",
    "    messages = [\n",
    "        HumanMessage(content=query)\n",
    "    ]\n",
    "    \n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    \n",
    "    # Check if function calling is needed\n",
    "    if response.tool_calls:\n",
    "        # Execute functions\n",
    "        tool_results = execute_tools(response)\n",
    "        \n",
    "        # Add function call and results to messages\n",
    "        messages.append(response)\n",
    "        messages.extend(tool_results)\n",
    "        \n",
    "        # Get final response\n",
    "        final_response = llm_with_tools.invoke(messages)\n",
    "        return final_response\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Test\n",
    "result = chat_with_tools(\"How much does pizza salami cost?\")\n",
    "print(\"Final result:\", result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Pydantic Classes for Tool Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'name': 'get_pizza_info', 'arguments': '{\"pizza_name\": \"Salami\"}'}} response_metadata={'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': [], 'model_provider': 'google_genai'} id='lc_run--019c9317-6c6a-7f70-9d6b-00128c576347-0' tool_calls=[{'name': 'get_pizza_info', 'args': {'pizza_name': 'Salami'}, 'id': 'bae0f4a8-d4d6-4330-8018-c75226daa9f9', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 67, 'output_tokens': 20, 'total_tokens': 87, 'input_token_details': {'cache_read': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "import json\n",
    "\n",
    "# Define a separate function for use with StructuredTool.from_function()\n",
    "# Note: We cannot use get_pizza_info here because it's already decorated with @tool\n",
    "def get_pizza_info_basic(pizza_name: str) -> str:\n",
    "    \"\"\"Get name and price of a pizza of the restaurant.\"\"\"\n",
    "    pizza_info = {\n",
    "        \"name\": pizza_name,\n",
    "        \"price\": \"10.99\",\n",
    "    }\n",
    "    return json.dumps(pizza_info)\n",
    "\n",
    "\n",
    "class GetPizzaInfo(BaseModel):\n",
    "    \"\"\"Get name and price of a pizza of the restaurant.\"\"\"\n",
    "    \n",
    "    pizza_name: str = Field(..., description=\"The name of the pizza, e.g. Salami\")\n",
    "\n",
    "\n",
    "# Convert Pydantic class to tool\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "get_pizza_info_tool = StructuredTool.from_function(\n",
    "    func=get_pizza_info_basic,\n",
    "    name=\"get_pizza_info\",\n",
    "    description=\"Get name and price of a pizza of the restaurant\",\n",
    "    args_schema=GetPizzaInfo\n",
    ")\n",
    "\n",
    "tools_from_pydantic = [get_pizza_info_tool]\n",
    "\n",
    "# Bind to LLM\n",
    "llm_with_pydantic_tools = llm.bind_tools(tools_from_pydantic)\n",
    "\n",
    "# Test\n",
    "result = llm_with_pydantic_tools.invoke(\"How much does pizza salami cost?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool output: {\"name\": \"Salami\", \"price\": \"10.99\"}\n"
     ]
    }
   ],
   "source": [
    "# Execute if tool calls are present\n",
    "if result.tool_calls:\n",
    "    for tool_call in result.tool_calls:\n",
    "        if tool_call[\"name\"] == \"get_pizza_info\":\n",
    "            output = get_pizza_info_tool.invoke(tool_call[\"args\"])\n",
    "            print(f\"Tool output: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
