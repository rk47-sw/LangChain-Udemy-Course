{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chains with multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the parrot get fired from the call center?\\n\\nBecause every time someone asked for technical support, he\\'d squawk, \"Polly wants a cracker! And maybe a new motherboard!\"'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "load_dotenv(find_dotenv())\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "# Basic chain with single input\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", \"Tell me a joke about {input}\")])\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"input\": \"a parrot\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hier ist ein Witz über einen Papagei auf Deutsch:\\n\\nEin Mann kommt in eine Tierhandlung und sieht einen schönen Papagei.\\nEr fragt den Verkäufer: \"Was kostet denn dieser Papagei?\"\\nDer Verkäufer antwortet: \"Dieser Papagei ist etwas Besonderes, er kann nämlich sprechen. Er kostet 1000 Euro.\"\\nDer Mann ist beeindruckt: \"Wirklich? Und was kann er sagen?\"\\nDer Verkäufer dreht sich zum Papagei und sagt: \"Na los, Hansi, sag mal was!\"\\nDer Papagei bleibt stumm.\\nDer Mann: \"Aber er sagt doch gar nichts!\"\\nDer Papagei blickt den Mann an und sagt: \"Ich bin doch nicht blöd! Wenn ich jetzt rede, verkauft der mich doch wieder für 1000 Euro!\"'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chain with multiple inputs\n",
    "prompt_template = ChatPromptTemplate.from_messages([(\"user\", \"Tell me a joke about {input} in {language}\")])\n",
    "chain = prompt_template | llm | StrOutputParser()\n",
    "chain.invoke({\"input\": \"a parrot\", \"language\": \"german\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains can be more complex and not all sequential chains will be as simple as passing a single string as an argument and getting a single string as output for all steps in the chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: \n",
      "Comment: \n",
      "Summary: \n",
      "German Translation: Die Rezension beschrieb die Pizza Salami als ein \"absolutes Desaster\" aufgrund ihres verbrannten/durchgeweichten Bodens, der fettigen Salami, des gummiartigen Käses und der wässrigen Soße, wodurch sie ungenießbar wurde.\n"
     ]
    }
   ],
   "source": [
    "# This is a chain to write a review given a dish name and the experience.\n",
    "prompt_review = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"You ordered {dish_name} and your experience was {experience}. Write a review:\")\n",
    "])\n",
    "\n",
    "# This is a chain to write a follow-up comment given the restaurant review.\n",
    "prompt_comment = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Given the restaurant review: {review}, write a follow-up comment:\")\n",
    "])\n",
    "\n",
    "# This is a chain to summarize a review.\n",
    "prompt_summary = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Summarise the review in one short sentence: {comment}\")\n",
    "])\n",
    "\n",
    "# This is a chain to translate a summary into German.\n",
    "prompt_translation = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Translate the summary to german: {summary}\")\n",
    "])\n",
    "\n",
    "# Create individual chains\n",
    "chain_review = prompt_review | llm | StrOutputParser()\n",
    "chain_comment = prompt_comment | llm | StrOutputParser()\n",
    "chain_summary = prompt_summary | llm | StrOutputParser()\n",
    "chain_translation = prompt_translation | llm | StrOutputParser()\n",
    "\n",
    "# Create overall sequential chain using LCEL\n",
    "def create_overall_chain():\n",
    "    def review_step(inputs):\n",
    "        return {\"review\": chain_review.invoke(inputs)}\n",
    "    \n",
    "    def comment_step(inputs):\n",
    "        return {\"comment\": chain_comment.invoke(inputs)}\n",
    "    \n",
    "    def summary_step(inputs):\n",
    "        return {\"summary\": chain_summary.invoke(inputs)}\n",
    "    \n",
    "    def translation_step(inputs):\n",
    "        return {\"german_translation\": chain_translation.invoke(inputs)}\n",
    "    \n",
    "    from langchain_core.runnables import RunnableLambda\n",
    "    \n",
    "    overall_chain = (\n",
    "        RunnableLambda(review_step) |\n",
    "        RunnableLambda(comment_step) |\n",
    "        RunnableLambda(summary_step) |\n",
    "        RunnableLambda(translation_step)\n",
    "    )\n",
    "    \n",
    "    return overall_chain\n",
    "\n",
    "overall_chain = create_overall_chain()\n",
    "result = overall_chain.invoke({\"dish_name\": \"Pizza Salami\", \"experience\": \"It was awful!\"})\n",
    "print(\"Review:\", result.get('review', ''))\n",
    "print(\"Comment:\", result.get('comment', ''))\n",
    "print(\"Summary:\", result.get('summary', ''))\n",
    "print(\"German Translation:\", result.get('german_translation', ''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of chaining multiple chains together we can also use an LLM to decide which follow up chain is being used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Destination chains created:\n",
      "- positive\n",
      "- neutral\n",
      "- negative\n"
     ]
    }
   ],
   "source": [
    "# Define templates for different sentiment analysis approaches\n",
    "positive_template = \"\"\"You are an AI that focuses on the positive side of things. \n",
    "Whenever you analyze a text, you look for the positive aspects and highlight them. \n",
    "Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "neutral_template = \"\"\"You are an AI that has a neutral perspective. You just provide a balanced analysis of the text, \n",
    "not favoring any positive or negative aspects. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "negative_template = \"\"\"You are an AI that is designed to find the negative aspects in a text. \n",
    "You analyze a text and show the potential downsides. Here is the text:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": \"positive\",\n",
    "        \"description\": \"Good for analyzing positive sentiments\",\n",
    "        \"prompt_template\": positive_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"neutral\",\n",
    "        \"description\": \"Good for analyzing neutral sentiments\",\n",
    "        \"prompt_template\": neutral_template,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"negative\",\n",
    "        \"description\": \"Good for analyzing negative sentiments\",\n",
    "        \"prompt_template\": negative_template,\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create destination chains\n",
    "destination_chains = {}\n",
    "for p_info in prompt_infos:\n",
    "    name = p_info[\"name\"]\n",
    "    prompt_template = p_info[\"prompt_template\"]\n",
    "    prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    destination_chains[name] = chain\n",
    "\n",
    "print(\"Destination chains created:\")\n",
    "for name in destination_chains:\n",
    "    print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Router chose: positive\n",
      "Result: That's wonderful to hear! Let's highlight the great things in your experience:\n",
      "\n",
      "*   **Awesome Pizza:** The most prominent positive is that the pizza was described as \"awesome!\" – that's fantastic!\n",
      "*   **Successful Order:** You successfully ordered exactly what you wanted, \"Pizza Salami.\"\n",
      "*   **Good Value:** At just $9.99, getting an \"awesome\" pizza suggests you received great value for your money.\n",
      "\n",
      "It sounds like you had a truly satisfying and delicious meal!\n"
     ]
    }
   ],
   "source": [
    "# Create router prompt\n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "router_template = \"\"\"Given a piece of text, determine which sentiment analysis approach to use. \n",
    "Here are the available options:\\n\"\"\" + destinations_str + \"\"\"\n",
    "\n",
    "Return the name of the destination chain to use (positive, neutral, or negative).\n",
    "Text: {input}\n",
    "Destination:\"\"\"\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_template(router_template)\n",
    "\n",
    "# Create router chain\n",
    "def router_function(inputs):\n",
    "    \"\"\"Route the input to the appropriate chain based on the router's decision.\"\"\"\n",
    "    # Get the router's decision\n",
    "    router_output = (router_prompt | llm | StrOutputParser()).invoke(inputs)\n",
    "    \n",
    "    # Extract the destination name from the router output\n",
    "    destination = router_output.strip().lower()\n",
    "    \n",
    "    # Default to neutral if the destination is not recognized\n",
    "    if destination not in destination_chains:\n",
    "        destination = \"neutral\"\n",
    "    \n",
    "    # Invoke the appropriate destination chain\n",
    "    result = destination_chains[destination].invoke(inputs)\n",
    "    return {\"result\": result, \"destination\": destination}\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "router_chain = RunnableLambda(router_function)\n",
    "\n",
    "# Test the router chain\n",
    "test_input = \"I ordered Pizza Salami for 9.99$ and it was awesome!\"\n",
    "result = router_chain.invoke({\"input\": test_input})\n",
    "print(f\"Router chose: {result['destination']}\")\n",
    "print(f\"Result: {result['result']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
